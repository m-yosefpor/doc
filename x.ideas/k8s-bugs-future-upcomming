# k8saf k8s-af
k8s known issues, which does not have any solution/workarounds yet:


KNI:
- one of the main issues of k8s to replace sth like openstack.
- having static IPs for pods

https://k8s.af


............. (mitigated)
- nf_conntrack -> increased a lot + monitoring/alert (many issues :D)
- ndot:5 -> node local caching + negative caching (no 0 ttl) + block AAAA (many issues), ndots4, use fqdn
- hpa -> never use
- clusterautoscaler -> never user
- automatic upgrade -> pin up to patch version (dnsmasq issue, terraform template issue, ksm issue)
- pod resource requests and limits -> clusterresourceoverride operator, 1:3 limit/req ratio
- pod priority and descheduler -> don't use.. also set quota and limit for ALL projects (even our infra) (fluentd preempted routers)
- create infra node roles which citical pods can fallback (or taint for critical, etc)
- scale-up consideration: be ware.. kubelet qos, multihashing, nfconntrack_hashsize_queue, haproxy reload scripts(many issues)
- pvc qos -> always use the ones which support qos
- previleged pods -> scc, userns
- http flood - backend app of routes-> rate limiting (we have not enforced yet)
- any flood - internal svc -> we have project network isolation
.................. (somehow / maybe mitigated)
- cfs throttling -> throttling dashboard/alerts, increase the request/limit (many issues), GOMAXPROC
- emptydir -> ephemeral-storage limit
- pod BW consumption - enforce on pod with mutation (?)
- many files pvc- docker freez: Docker stucks in chowing lots of files (minio) ->  use crio: SELINUX: https://github.com/kubernetes/enhancements/issues/1710
  fsgroupChnagePolicy: onRootMismatch
  SELINUX policy
  higher iops
- many files pvc- long startup (recursive chowning, chmoding and chconing! (minio,jira,confluence:
  SELINUX: https://github.com/kubernetes/enhancements/issues/1710
  enforce OnRootMismatch
................. (no solution yet)
- pod io wait, cause node highload avg (devops backup)
- many k8s operations (controller,scheduler,api): Don't run your cronjob every minute! (ode)
- dns flood (eck-operator) -> node local? OOM/throttled to send to central .. whatabout per pod?
- http flood - api-server
- http flood - router (smapp app)
- packet flood - ovs, node
- disk io qos in k8s layer (cgroup v2)
- cinder state stuck in reserved/detaching.. what if ceph-csi (what about qos?)?
- metric cardinality scaling
............................ (always there)
- major changes -> release processes: staging, canary, approval, rollback/backup

............................. (??)
etcd-number-of-objects/size of each, space quota, etc
- users set taint?

security:
- userns -> anyuid
- cgroup ns
- cgroup v2
-




Namespaced CRDs:
- users can not write and use operators (it's ok I think)
- multiple operators (with different CRD/version) will collide even if they have limited to some namespaces lig OperatorGroup






many of operators does not emmit enough events, instead they only log (appset controller, ...) which is not possible for users to debug, only requires a cluster-admin then.






- api changes (renames)
- big nodes: kubelet , draining https://learnk8s.io/kubernetes-node-size

upcomming:
  security:
  - userns -> anyuid
  - cgroup ns
  - cgroup v2
  burstable cfs
  qos on network in podSpec (annotation?)
  qos on blkio in podSpec (cgroupv2)
  - ephermeral containers for debugging distroless containers


k8s issues public:

generating many events and making etcd full
high api load, or deploy pod creation, job creation, large ep, fast route/ep changing (causing high reloads), etc
